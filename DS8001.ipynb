{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization of segments of the end to end ensemble training and evaluation for a text classification task\n",
    "\n",
    "The problem taken from this [Kaggle competition](https://www.kaggle.com/c/spooky-author-identification).\n",
    "Data can be downloaded from [here](https://www.kaggle.com/c/spooky-author-identification/data). The `train.csv` and `test.csv` must be present in the same directory as this notebook.\n",
    "Also, download the Glove embeddings file `glove.840B.300d.zip` from [here](https://nlp.stanford.edu/projects/glove/), unzip it and keep in the same directory. One final thing you need to do is to split this file into 4 equal segments called `glove_part1.txt`, `glove_part2.txt`, `glove_part3.txt` and `glove_part4.txt`. We have done it using unix's `split` command\n",
    "\n",
    "```\n",
    "split -l 549005 glove.840B.300d.txt\n",
    "```\n",
    "and then renamed the parts like mentioned. These 4 split files also have to be in the same directory.\n",
    "Once you have these, you can run this notebook in a pyhton 3 virtual environment where you have the following packages installed: `pandas, numpy, matplotlib, nltk, sklearn, keras, jupyter`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack as sp_hstack, vstack as sp_vstack\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout, Layer, K\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "training_set = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7900, 3)\n",
      "(5635, 3)\n",
      "(6044, 3)\n"
     ]
    }
   ],
   "source": [
    "print(training_set[training_set.author == 'EAP'].shape)\n",
    "print(training_set[training_set.author == 'HPL'].shape)\n",
    "print(training_set[training_set.author == 'MWS'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(training_set, stratify=training_set['author'], random_state=20, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train['text']\n",
    "y_train = df_train['author']\n",
    "x_val = df_val['text']\n",
    "y_val = df_val['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the label values in Keras ready format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "y_train = label_enc.fit_transform(y_train)\n",
    "y_val = label_enc.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility method for getting succesive chunks of data. This is handy method for the data parallelization stuffs used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_chunk(data):\n",
    "    for i in range(4):\n",
    "        yield data[math.ceil(i * len(data) / 4):math.ceil((i + 1) * len(data) / 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loss measure function has been taken from [this Kaggle kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download some necessary nltk stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility methods for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def normalize(text):\n",
    "   words = word_tokenize(text) \n",
    "   words = [w for w in words if not w in stop_words]\n",
    "\n",
    "   stemw = [ls.stem(w) for w in words]\n",
    "   \n",
    "   # 2- Lemmatization\n",
    "   lemw = [lem.lemmatize(w) for w in stemw]\n",
    "   return ' '.join(lemw)\n",
    "\n",
    "def get_preprocessed_data(data):\n",
    "    data = data.apply(normalize)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thi process , howev , afford mean ascertain dimend dungeon ; i might mak circuit , return point whent i set , without aw fact ; perfect uniform seem wal .'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential pre_processing of training data: 6.91552209854126 seconds\n"
     ]
    }
   ],
   "source": [
    "start_preprocess = time.time()\n",
    "x_train_preprocessed = get_preprocessed_data(x_train)\n",
    "end_preprocess = time.time()\n",
    "time_preprocess_seq = end_preprocess - start_preprocess\n",
    "print('Time taken by sequential pre_processing of training data: {} seconds'.format(time_preprocess_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.9s remaining:    1.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallellizing pre_processing of training data: 2.296858787536621 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "start_preprocess = time.time()\n",
    "x_train_preprocessed = np.hstack(Parallel(n_jobs=4, backend='loky', verbose=10)\\\n",
    "                            (delayed(get_preprocessed_data)(data) for data in next_chunk(x_train)))\n",
    "\n",
    "end_preprocess = time.time()\n",
    "time_preprocess_par = end_preprocess - start_preprocess\n",
    "print('Time taken by parallellizing pre_processing of training data: {} seconds'.format(time_preprocess_par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization helped for pre-processing as seen sbove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_preprocessed = get_preprocessed_data(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=1,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=1,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "tfidf_vectorizer.fit(x_train_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential tfidf calculation of training data: 0.39216184616088867 seconds\n"
     ]
    }
   ],
   "source": [
    "start_tfidf = time.time()\n",
    "x_train_prep_tfidf = tfidf_vectorizer.transform(x_train_preprocessed)\n",
    "end_tfidf = time.time()\n",
    "time_tfidf_seq = end_tfidf - start_tfidf\n",
    "print('Time taken by sequential tfidf calculation of training data: {} seconds'.format(time_tfidf_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.5s remaining:    2.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel tfidf calculation of training data: 4.662719249725342 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    4.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "start_tfidf = time.time()\n",
    "x_train_prep_tfidf = sp_vstack(Parallel(n_jobs=4, backend='loky', verbose=10)\\\n",
    "                            (delayed(tfidf_vectorizer.transform)(data) for data in next_chunk(x_train_preprocessed)))\n",
    "\n",
    "end_tfidf = time.time()\n",
    "time_tfidf_par = end_tfidf - start_tfidf\n",
    "print('Time taken by parallel tfidf calculation of training data: {} seconds'.format(time_tfidf_par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization did not help for the tfidf calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_prep_tfidf = tfidf_vectorizer.transform(x_val_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\deep_learning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "d:\\envs\\deep_learning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=11.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_model = LogisticRegression(C=11.0)\n",
    "logreg_model.fit(x_train_prep_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7913687436159347"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_model.score(x_val_prep_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(x_train_prep_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911133810010215"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.score(x_val_prep_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = RandomForestClassifier(n_estimators=100, random_state=1, min_samples_leaf=3, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.fit(x_train_prep_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017364657814096"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.score(x_val_prep_tfidf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the 3 models (Logistic Regression, Naive Bayes and Random Forest) sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\deep_learning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "d:\\envs\\deep_learning\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential run of ensemble model: 4.455329418182373 seconds\n"
     ]
    }
   ],
   "source": [
    "models = [logreg_model, rfmodel, nb_model]\n",
    "\n",
    "def fit_and_evaluate_model(model, x_train, y_train, x_val, y_val):\n",
    "    model.fit(x_train, y_train)\n",
    "    probs = model.predict_proba(x_val)\n",
    "    score = model.score(x_val, y_val)\n",
    "    return probs, score\n",
    "\n",
    "start_ensemble = time.time()\n",
    "results = []\n",
    "for model in models:\n",
    "    results.append(fit_and_evaluate_model(model, x_train_prep_tfidf, y_train, x_val_prep_tfidf, y_val))\n",
    "end_ensemble = time.time()\n",
    "time_ensemble_seq = end_ensemble - start_ensemble\n",
    "print('Time taken by sequential run of ensemble model: {} seconds'.format(time_ensemble_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Parallellize the same training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel run of ensemble model: 3.4047114849090576 seconds\n"
     ]
    }
   ],
   "source": [
    "start_ensemble = time.time()\n",
    "results = Parallel(n_jobs=3, backend='loky', verbose=0)\\\n",
    "                            (delayed(fit_and_evaluate_model)(model, x_train_prep_tfidf, y_train, x_val_prep_tfidf, y_val) \\\n",
    "                             for model in models)\n",
    "end_ensemble = time.time()\n",
    "time_ensemble_par = end_ensemble - start_ensemble\n",
    "print('Time taken by parallel run of ensemble model: {} seconds'.format(time_ensemble_par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization helped for the ensemble model run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential reading of the embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential embeddings read: 153.56502962112427 seconds\n"
     ]
    }
   ],
   "source": [
    "# The code for utility function read_embeddings is taken from:\n",
    "# https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb\n",
    "def read_embeddings(file):\n",
    "    embeddings_index = {}\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "       for line in f:\n",
    "           values = line.split(' ')\n",
    "           word = values[0]\n",
    "           coefs = np.asarray(values[1:], dtype='float32')\n",
    "           embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "start_embedding = time.time()\n",
    "read_embeddings('glove.840B.300d.txt')\n",
    "end_embedding = time.time()\n",
    "time_embedding_seq = end_embedding - start_embedding\n",
    "print('Time taken by sequential embeddings read: {} seconds'.format(time_embedding_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embeddings file is huge in size - almost 5.5 GB. So we have splitted the file into 4 parts so that it can be read in parallel by 4 worker process working on 4 processor cores. \n",
    "### Parallel reading of the embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  2.2min remaining:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel embeddings read: 111.45695376396179 seconds\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "start_embedding = time.time()\n",
    "files = ['glove_part1.txt', 'glove_part2.txt', 'glove_part3.txt', 'glove_part4.txt']\n",
    "indices = Parallel(n_jobs=4, backend='loky', verbose=10)\\\n",
    "                            (delayed(read_embeddings)(file) for file in files)\n",
    "embeddings_index = {}\n",
    "for index in indices:\n",
    "    embeddings_index.update(index)\n",
    "end_embedding = time.time()\n",
    "\n",
    "#time_embedding_par = end_embedding - start_embedding\n",
    "print('Time taken by parallel embeddings read: {} seconds'.format(time_embedding_par))\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization helped for reading embeddings file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11776 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "#max_words = 10000\n",
    "# using keras tokenizer here\n",
    "tokenizer = text.Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(x_train_preprocessed)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for embedding_matrix is taken from:\n",
    "# https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "   embedding_vector = embeddings_index.get(word)\n",
    "   if embedding_vector is not None:\n",
    "       embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility method for sequencing senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_data(data):\n",
    "   data = tokenizer.texts_to_sequences(data)\n",
    "   # zero pad the sequences\n",
    "   data = sequence.pad_sequences(data, maxlen=max_len)\n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential execution of sentence sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential tokenization and sequencing: 33.29940700531006 seconds\n"
     ]
    }
   ],
   "source": [
    "start_sequence = time.time()\n",
    "x_train_sequence = get_sequence_data(x_train_preprocessed)\n",
    "end_sequence = time.time()\n",
    "time_sequence_seq = end_sequence - start_sequence\n",
    "print('Time taken by sequential tokenization and sequencing: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel execution of sentence sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel tokenization and sequencing: 24.17801570892334 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   24.0s remaining:   24.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   24.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   24.1s finished\n"
     ]
    }
   ],
   "source": [
    "start_sequence = time.time()\n",
    "x_train_sequence = np.vstack(Parallel(n_jobs=4, backend='loky', verbose=10)\\\n",
    "                            (delayed(get_sequence_data)(data) for data in next_chunk(x_train_preprocessed)))\n",
    "end_sequence = time.time()\n",
    "time_sequence_par = end_sequence - start_sequence\n",
    "print('Time taken by parallel tokenization and sequencing: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization helped for tokenizing and sequencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_sequence = get_sequence_data(x_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = np_utils.to_categorical(y_train)\n",
    "y_val_enc = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of neural models (RNN with LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(dropout=0.3):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                        300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(LSTM(100, dropout=dropout, recurrent_dropout=dropout))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fit_and_evaluate_nn_model(model, x_train, y_train, x_val, y_val, batch_size=512):\n",
    "    model.fit(x_train, y=y_train, batch_size=batch_size, epochs=10, verbose=1)\n",
    "    probs = model.predict(x_val)\n",
    "    _, score = model.evaluate(x_val, y_val)\n",
    "    return probs, score\n",
    "\n",
    "def get_nn_models():\n",
    "    return [nn_model(0.3), nn_model(0.4), nn_model(0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential execution of the training of 3 different neural models (with 3 different dropout rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 57s 4ms/step - loss: 1.0283 - acc: 0.4702\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 60s 4ms/step - loss: 0.9346 - acc: 0.5542\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.8983 - acc: 0.5849\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 65s 4ms/step - loss: 0.8811 - acc: 0.5974\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.8642 - acc: 0.6006\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 61s 4ms/step - loss: 0.8478 - acc: 0.6156\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 61s 4ms/step - loss: 0.8182 - acc: 0.6296\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.8076 - acc: 0.6342\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.7814 - acc: 0.6553\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.7573 - acc: 0.6693\n",
      "3916/3916 [==============================] - 7s 2ms/step\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 1.0500 - acc: 0.4415\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.9634 - acc: 0.5339\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 60s 4ms/step - loss: 0.9256 - acc: 0.5650\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.9115 - acc: 0.5713\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.8938 - acc: 0.5885\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.8763 - acc: 0.5978\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.8637 - acc: 0.6026\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 61s 4ms/step - loss: 0.8476 - acc: 0.6168\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.8289 - acc: 0.6256\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 61s 4ms/step - loss: 0.8076 - acc: 0.6379\n",
      "3916/3916 [==============================] - 7s 2ms/step\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 1.0600 - acc: 0.4328\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.9921 - acc: 0.5035\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 67s 4ms/step - loss: 0.9573 - acc: 0.5351\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 64s 4ms/step - loss: 0.9388 - acc: 0.5534\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 62s 4ms/step - loss: 0.9318 - acc: 0.5650\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 66s 4ms/step - loss: 0.9195 - acc: 0.5683\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.9078 - acc: 0.5782\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 61s 4ms/step - loss: 0.8905 - acc: 0.5876\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 61s 4ms/step - loss: 0.8765 - acc: 0.6003\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 63s 4ms/step - loss: 0.8675 - acc: 0.6021\n",
      "3916/3916 [==============================] - 7s 2ms/step\n",
      "Time taken by sequential training of 3 models: 2127.9264843463898 seconds\n"
     ]
    }
   ],
   "source": [
    "start_ensemble_nn = time.time()\n",
    "nn_results = []\n",
    "models_seq = get_nn_models()\n",
    "for model in models_seq:\n",
    "    nn_results.append(fit_and_evaluate_nn_model(model, x_train_sequence, y_train_enc, x_val_sequence, y_val_enc))\n",
    "end_ensemble_nn = time.time()\n",
    "time_ensemble_nn_seq = end_ensemble_nn - start_ensemble_nn\n",
    "print('Time taken by sequential training of 3 models: {} seconds'.format(time_ensemble_nn_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel execution of the training of same 3 models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallellizing training of 3 models: 809.9761328697205 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed: 13.8min finished\n"
     ]
    }
   ],
   "source": [
    "start_ensemble_nn = time.time()\n",
    "models_par = get_nn_models()\n",
    "nn_results = Parallel(n_jobs=3, backend='loky', verbose=1)\\\n",
    "                            (delayed(fit_and_evaluate_nn_model)(model, x_train_sequence, y_train_enc, \n",
    "                                                                x_val_sequence, y_val_enc) \\\n",
    "                             for model in models_par)\n",
    "end_ensemble_nn = time.time()\n",
    "time_ensemble_nn_par = end_ensemble_nn - start_ensemble_nn\n",
    "print('Time taken by parallellizing training of 3 models: {} seconds'.format(time_ensemble_nn_par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization helped for neural model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127.9264843463898"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_ensemble_nn_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809.9761328697205"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_ensemble_nn_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Results for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_set['text']\n",
    "x_test_preprocessed = get_preprocessed_data(x_test)\n",
    "x_test_prep_tfidf = tfidf_vectorizer.transform(x_test_preprocessed)\n",
    "x_test_sequence = get_sequence_data(x_test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential execution of probability prediction of the 6 models we got so far (3 shallow and 3 deep neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential ensemble result calculation: 35.67901802062988 seconds\n"
     ]
    }
   ],
   "source": [
    "start_ensemble_result = time.time()\n",
    "ensemble_probs = models[0].predict_proba(x_test_prep_tfidf)\n",
    "for model in models[1:]:\n",
    "    ensemble_probs += model.predict_proba(x_test_prep_tfidf)\n",
    "for model in models_par:\n",
    "    ensemble_probs += model.predict(x_test_sequence)\n",
    "ensemble_probs /= 6\n",
    "end_ensemble_result = time.time()\n",
    "time_ensemble_result_seq = end_ensemble_result - start_ensemble_result\n",
    "print('Time taken by sequential ensemble result calculation: {} seconds'.format(time_ensemble_result_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel execution of probability prediction of the 6 models we got so far (3 shallow and 3 deep neural). We let the shallow models still predict sequentiall since they are pretty fast and parallelization doesn't help too much. However we run the deep models' predicition in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel ensemble result calculation: 14.850965023040771 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:   14.3s finished\n"
     ]
    }
   ],
   "source": [
    "start_ensemble_result = time.time()\n",
    "\n",
    "ensemble_probs = models[0].predict_proba(x_test_prep_tfidf)\n",
    "for model in models[1:]:\n",
    "    ensemble_probs += model.predict_proba(x_test_prep_tfidf)\n",
    "    \n",
    "ensemble_results_deep = Parallel(n_jobs=3, backend='loky', verbose=1)\\\n",
    "                            (delayed(model.predict)(x_test_sequence) \\\n",
    "                             for model in models_par)\n",
    "for result in ensemble_results_deep:\n",
    "    ensemble_probs +=result\n",
    "ensemble_probs /= 6\n",
    "\n",
    "end_ensemble_result = time.time()\n",
    "time_ensemble_result_par = end_ensemble_result - start_ensemble_result\n",
    "print('Time taken by parallel ensemble result calculation: {} seconds'.format(time_ensemble_result_par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization of the neural models helped improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our most significant performance gain came with parallelizing the deep model training step. We will try to visualize it more deeply using traing data of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [3000, 6000, 9000, 12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0813 - acc: 0.4173\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0515 - acc: 0.4663\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 1.0135 - acc: 0.4800\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 0.9778 - acc: 0.5213\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 0.9563 - acc: 0.5337\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 0.9219 - acc: 0.5593\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9195 - acc: 0.5603\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 0.8873 - acc: 0.5787\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.8642 - acc: 0.6010\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.8602 - acc: 0.6073\n",
      "3916/3916 [==============================] - 10s 2ms/step\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 1.0982 - acc: 0.3960\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0771 - acc: 0.4220\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 1.0511 - acc: 0.4370\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0225 - acc: 0.4867\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 1.0055 - acc: 0.5027\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9714 - acc: 0.5213\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9378 - acc: 0.5410\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9411 - acc: 0.5477\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9164 - acc: 0.5663\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 0.9097 - acc: 0.5790\n",
      "3916/3916 [==============================] - 11s 3ms/step\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 1.0913 - acc: 0.4160\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0797 - acc: 0.4130\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0560 - acc: 0.4417\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0320 - acc: 0.4780\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 1.0156 - acc: 0.4880\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0110 - acc: 0.4993\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9745 - acc: 0.5377\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9626 - acc: 0.5367\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9557 - acc: 0.5307\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.9437 - acc: 0.5450\n",
      "3916/3916 [==============================] - 11s 3ms/step\n",
      "Time taken by sequential training of 3 models with size 3000: 458.0243697166443 seconds\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.0735 - acc: 0.4138\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 27s 4ms/step - loss: 1.0150 - acc: 0.4802\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.9529 - acc: 0.5418\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.9207 - acc: 0.5638\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.8910 - acc: 0.5813\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 27s 4ms/step - loss: 0.8678 - acc: 0.6032\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 27s 4ms/step - loss: 0.8769 - acc: 0.5903\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.8513 - acc: 0.6067\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 26s 4ms/step - loss: 0.8341 - acc: 0.6147\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 27s 4ms/step - loss: 0.8371 - acc: 0.6217\n",
      "3916/3916 [==============================] - 11s 3ms/step\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 30s 5ms/step - loss: 1.0817 - acc: 0.4140\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 27s 4ms/step - loss: 1.0341 - acc: 0.4630\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9853 - acc: 0.5180\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 27s 5ms/step - loss: 0.9587 - acc: 0.5360\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 27s 5ms/step - loss: 0.9332 - acc: 0.5548\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 27s 5ms/step - loss: 0.9217 - acc: 0.5597\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9098 - acc: 0.5720\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 27s 5ms/step - loss: 0.8922 - acc: 0.5830\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 27s 5ms/step - loss: 0.8842 - acc: 0.5933\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.8755 - acc: 0.5975\n",
      "3916/3916 [==============================] - 13s 3ms/step\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 31s 5ms/step - loss: 1.0834 - acc: 0.4040\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.0558 - acc: 0.4388\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 1.0193 - acc: 0.4825\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9860 - acc: 0.5070\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9658 - acc: 0.5295\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9514 - acc: 0.5418\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 27s 5ms/step - loss: 0.9382 - acc: 0.5492\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9217 - acc: 0.5637\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9339 - acc: 0.5577\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 28s 5ms/step - loss: 0.9170 - acc: 0.5645\n",
      "3916/3916 [==============================] - 13s 3ms/step\n",
      "Time taken by sequential training of 3 models with size 6000: 910.4481463432312 seconds\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 1.0662 - acc: 0.4209\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 42s 5ms/step - loss: 0.9885 - acc: 0.5039\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 42s 5ms/step - loss: 0.9296 - acc: 0.5636\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.9061 - acc: 0.5788\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8850 - acc: 0.5911\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8679 - acc: 0.6002\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8500 - acc: 0.6123\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8568 - acc: 0.6092\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8241 - acc: 0.6271\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 42s 5ms/step - loss: 0.8163 - acc: 0.6294\n",
      "3916/3916 [==============================] - 14s 4ms/step\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 1.0699 - acc: 0.4249\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.9969 - acc: 0.5080\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.9648 - acc: 0.5337\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.9359 - acc: 0.5578\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.9147 - acc: 0.5748\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.9044 - acc: 0.5782\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8827 - acc: 0.5890\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 43s 5ms/step - loss: 0.8804 - acc: 0.5927\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.8668 - acc: 0.6007\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 44s 5ms/step - loss: 0.8581 - acc: 0.6077\n",
      "3916/3916 [==============================] - 13s 3ms/step\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 49s 5ms/step - loss: 1.0837 - acc: 0.3928\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 1.0298 - acc: 0.4693\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 0.9915 - acc: 0.5101\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 0.9696 - acc: 0.5290\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 47s 5ms/step - loss: 0.9510 - acc: 0.5401\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 46s 5ms/step - loss: 0.9338 - acc: 0.5561\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 0.9313 - acc: 0.5586\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 0.9134 - acc: 0.5681\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 0.9131 - acc: 0.5709\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 45s 5ms/step - loss: 0.8996 - acc: 0.5817\n",
      "3916/3916 [==============================] - 14s 4ms/step\n",
      "Time taken by sequential training of 3 models with size 9000: 1454.1849617958069 seconds\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 67s 6ms/step - loss: 1.0566 - acc: 0.4352\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 66s 5ms/step - loss: 0.9582 - acc: 0.5347\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 65s 5ms/step - loss: 0.9108 - acc: 0.5775\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 64s 5ms/step - loss: 0.8880 - acc: 0.5853\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 64s 5ms/step - loss: 0.8684 - acc: 0.6049\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 65s 5ms/step - loss: 0.8522 - acc: 0.6163\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 65s 5ms/step - loss: 0.8432 - acc: 0.6180\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 66s 5ms/step - loss: 0.8203 - acc: 0.6239\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 65s 5ms/step - loss: 0.7975 - acc: 0.6449\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 64s 5ms/step - loss: 0.7801 - acc: 0.6495\n",
      "3916/3916 [==============================] - 17s 4ms/step\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 1.0585 - acc: 0.4308\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.9818 - acc: 0.5171\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.9306 - acc: 0.5527\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.9136 - acc: 0.5728\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.9044 - acc: 0.5746\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.8874 - acc: 0.5936\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 68s 6ms/step - loss: 0.8765 - acc: 0.5957\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.8637 - acc: 0.6017\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 67s 6ms/step - loss: 0.8464 - acc: 0.6138\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 68s 6ms/step - loss: 0.8312 - acc: 0.6212\n",
      "3916/3916 [==============================] - 23s 6ms/step\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 72s 6ms/step - loss: 1.0760 - acc: 0.4157\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 1.0075 - acc: 0.4913\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 73s 6ms/step - loss: 0.9700 - acc: 0.5236\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.9551 - acc: 0.5353\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 68s 6ms/step - loss: 0.9449 - acc: 0.5485\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.9286 - acc: 0.5581\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 73s 6ms/step - loss: 0.9167 - acc: 0.5695\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.9081 - acc: 0.5813\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.8981 - acc: 0.5827\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.8828 - acc: 0.5910\n",
      "3916/3916 [==============================] - 27s 7ms/step\n",
      "Time taken by sequential training of 3 models with size 12000: 2253.602767944336 seconds\n"
     ]
    }
   ],
   "source": [
    "times_nn_seq = []\n",
    "for i, size in enumerate(sizes):\n",
    "    start_ensemble_nn = time.time()\n",
    "    models_seqq = get_nn_models()\n",
    "    for model in models_seqq:\n",
    "        fit_and_evaluate_nn_model(model, x_train_sequence[:size], y_train_enc[:size], x_val_sequence, y_val_enc)\n",
    "    end_ensemble_nn = time.time()\n",
    "    times_nn_seq.append(end_ensemble_nn - start_ensemble_nn)\n",
    "    print('Time taken by sequential training of 3 models with size {}: {} seconds'.format(size, times_nn_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel training of 3 models with size 3000: 178.8060863018036 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel training of 3 models with size 6000: 335.29078340530396 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel training of 3 models with size 9000: 487.3409073352814 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel training of 3 models with size 12000: 640.6674711704254 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed: 10.4min finished\n"
     ]
    }
   ],
   "source": [
    "times_nn_parr = []\n",
    "for i, size in enumerate(sizes):\n",
    "    start_ensemble_nn = time.time()\n",
    "    models_parr = get_nn_models()\n",
    "    Parallel(n_jobs=3, backend='loky', verbose=1)\\\n",
    "                            (delayed(fit_and_evaluate_nn_model)(model, x_train_sequence[:size], y_train_enc[:size], \n",
    "                                                                x_val_sequence, y_val_enc) \\\n",
    "                             for model in models_parr)\n",
    "    end_ensemble_nn = time.time()\n",
    "    times_nn_parr.append(end_ensemble_nn - start_ensemble_nn)\n",
    "    print('Time taken by parallel training of 3 models with size {}: {} seconds'.format(size, times_nn_parr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_nn_seq.append(time_ensemble_nn_seq)\n",
    "times_nn_parr.append(time_ensemble_nn_par)\n",
    "sizes.append(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interplotate_smooth(x, y):\n",
    "    x_new = np.linspace(min(x) - 100, max(x) + 100, 300)\n",
    "\n",
    "    spl = make_interp_spline(x, y, k=3)\n",
    "    y_new = spl(x_new)\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the execution time trend against data size for the deep models training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*interplotate_smooth(sizes, times_nn_seq), label='Sequential')\n",
    "plt.plot(*interplotate_smooth(sizes, times_nn_parr), label='Parallel')\n",
    "plt.xlabel('Number of data points')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Training of deep neural network models ')\n",
    "plt.legend()\n",
    "plt.savefig('neural_seq_vs_par.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will also try to experiment and visualize the second most performance gaining step of our implementation, which is the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by sequential preprocessing with size 3000: 1.348273515701294 seconds\n",
      "Time taken by sequential preprocessing with size 6000: 2.667013168334961 seconds\n",
      "Time taken by sequential preprocessing with size 9000: 4.019038200378418 seconds\n",
      "Time taken by sequential preprocessing with size 12000: 5.396446943283081 seconds\n"
     ]
    }
   ],
   "source": [
    "times_prep_seq = []\n",
    "for i, size in enumerate(sizes[:4]):\n",
    "    start_preprocess = time.time()\n",
    "    temp = get_preprocessed_data(x_train[:size])\n",
    "    end_preprocess = time.time()\n",
    "    times_prep_seq.append(end_preprocess - start_preprocess)\n",
    "    print('Time taken by sequential preprocessing with size {}: {} seconds'.format(size, times_prep_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_prep_seq.append(time_preprocess_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel preprocessing with size 3000: 2.2475900650024414 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel preprocessing with size 6000: 2.2816946506500244 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.0s remaining:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel preprocessing with size 9000: 2.1778769493103027 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel preprocessing with size 12000: 2.408221960067749 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.3s remaining:    2.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by parallel preprocessing with size 15663: 2.5456459522247314 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "times_prep_parr = []\n",
    "for i, size in enumerate(sizes[:5]):\n",
    "    start_preprocess = time.time()\n",
    "    temp = np.hstack(Parallel(n_jobs=4, backend='loky', verbose=10)\\\n",
    "                            (delayed(get_preprocessed_data)(data) for data in next_chunk(x_train)))\n",
    "    end_preprocess = time.time()\n",
    "    times_prep_parr.append(end_preprocess - start_preprocess)\n",
    "    print('Time taken by parallel preprocessing with size {}: {} seconds'.format(size, times_prep_parr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the execution time trend against data size for the preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*interplotate_smooth(sizes, times_prep_seq), label='Sequential')\n",
    "plt.plot(*interplotate_smooth(sizes, times_prep_parr), label='Parallel')\n",
    "plt.xlabel('Number of data points')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Preprocessing of texts')\n",
    "plt.legend()\n",
    "plt.savefig('prep_seq_vs_par.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
